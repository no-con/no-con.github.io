???+ Abstract

    简单介绍了机器学习及机器学习类型，介绍了决策树和集成机器学习的含义，以及随机森林。
    
    决策树的含义，最佳属性选择策略（信息熵和信息增益）；随机森林的含义、思想和优缺点。

## 1. 机器学习

### 1.1 机器学习介绍

机器学习是人工智能和计算机科学的一个分支，它侧重于使用数据和算法来模仿人类的学习方式，逐步提高其准确性。

* 机器学习是不断发展着的数据科学领域的重要组成部分。

* 通过使用统计方法，训练算法进行分类或预测，揭示数据挖掘项目中的关键见解。

* 这些见解随后推动应用程序和业务中的决策制定，理想情况下影响关键指标。

人工神经网络相关层次关系：

> 人工智能（Artificial Intelligence）  
> 
> > 机器学习（Machine Learning）
> > 
> > > 深度学习（Deep Learning）
> > > 
> > > > 神经网络 （Neural Network）
> > > > 
> > > > > 卷积神经网络(Convolutional Neural Networks)

### 1.2 机器学习主要内容

* 决策过程
  
    一般来说，机器学习算法用于预测或分类。基于一些输入数据（有标注或未标注），算法将估计数据中存在的模式。

* 误差函数
  
    误差函数用于评价模型的预测结果。

* 模型优化
  
    调整模型权重以减少模型预测结果与真实结果之间的差异。

### 1.3 机器学习的分类

* 监督机器学习
  
    使用有标注数据训练模型算法。
  
    当输入数据输入到模型中，模型会调整其权重直到与输入数据适当拟合。
  
    一些监督学习算法：神经网络、朴素贝叶斯、[线性回归](../7_Regression/#2)、逻辑斯蒂回归、[随机森林](#2)、支持向量机 …

* 无监督机器学习
  
    对无标记数据进行分析和聚类。无需人工干预即可发现数据中隐藏的模式和数据分组。
  
    无监督学习算法也用于通过降维过程减少模型中的特征数量，常见的算法是主成分分析(PCA)和奇异值分解(SVD)。
  
    一些无监督学习算法：神经网络、[k-means聚类](../9_SpaCluster/#31)、概率聚类方法 …

* 半监督机器学习
  
    使用较小的标记数据集指导较大的无标记数据集进行分类和特征提取。

## 2. 随机森林分类

### 2.1 决策树

决策树是一种使用树状模型的决策支持工具，是只包含条件控制语句算法的显示方式，常用于**图像分类**、运筹学、运营管理等。

![decision](img/decision%20tree.png){.img}

**决策树的节点**：

* 内部节点（Internel node）：根据节点对应的属性将数据分为不同的分支。  
* 叶节点（Terminal/Leaf node）：输入数据最后分为的类别。

!!! note inline end

    在本文中，属性、变量、要素特征具有相同的含义。

**决策树的构建**（自上而下、贪婪搜索）：

1. 选择作为树根节点的最佳属性；  
2. 根据节点的属性将训练数据集D分为不同的非空互补子集，每个子集的对应属性值均相同；  
3. 递归对每个子集应用该算法，直至训练样例具有相同的类别。

**最佳属性的选择**：

* 有很多方法可用于对数据分组，不同方法都有其优缺点；  
* 可以通过比较不同属性/特征的分类结果的复杂度来评价该分类方法的效果；  
* 不同决策树算法主要是它们的属性选择策略不同。

**一种典型的决策树算法——ID3**

* 是C4.5和C5.0算法的基础；  
* 使用信息熵和信息增益确定划分属性。

**信息熵(Information Entropy)**

信息熵表示了数据中的不确定性、信息量水平。熵越大，数据的不确定性越高，数据越复杂。

假设数据集 S 有k个类别，第i个类别的占比为$p_i$，则数据集 S 的信息熵为：
$$
Entropy(S) = - \sum_{i=1}^k p_i log_2 (p_i)
$$

**信息增益(Information Gain)**

通过属性A划分前后数据集S的信息熵之间的差。在决策树中信息增益衡量了选择的属性对数据集进行划分后减少的数据集不确定度。
$$
IG(S, A) = E(S) - \sum_{v}^V \frac{S_v}{S} E(S_v) = E(S) - \sum_{v}^V \frac{S_v}{S} \sum_{k=1}^K \frac{S_{vk}}{S_v}log_2 \frac{S_{vk}}{S_v}
$$
其中，$E(S)$ 是所有样例的信息熵（对数据集进行划分之前），$E(S_v)$是选择属性后对数据划分后子集$S_v$的信息熵。  

信息增益越大，选择的划分特征越好。

**过拟合(Overfitting)**

如果训练决策树时让树长得太深，使得其开始探捕捉数据集中的偏差，可能将噪声点也进行划分了，导致其预测新数据的能力反而减弱。

如何解决决策树的过拟合问题：

1. 在过拟合树之前停止继续生成决策树；    
    在实际操作中很难实现，因为我们无法知道合适的停止时机是什么。

2. 让决策树不断生长，直至结束，在后处理步骤进行**剪枝(prune)**。   

!!! tip "机器学习中的过拟合与欠拟合"

    过拟合（Overfitting）和欠拟合（Underfitting）是机器学习中两种常见的模型训练问题。
    
    过拟合是指模型在训练数据集上表现好，在测试数据集上表现差。过拟合通常发生在模型过于复杂、参数过多的情况下，导致模型过度记忆了训练数据的噪声和细节，而忽略了数据的整体趋势和泛化能力。过拟合的模型对训练数据过于敏感，可能会出现过度拟合噪声的情况，导致泛化能力下降。
    
    欠拟合是指模型在训练数据集上表现差（，在测试数据集上表现也差）。欠拟合通常发生在模型过于简单、参数过少的情况下，导致模型无法适应数据的复杂性和变化，无法捕捉数据的潜在规律和特征。欠拟合的模型可能会出现高偏差的情况，导致无法充分拟合训练数据，也无法在测试数据上取得良好的性能。
    
    ---
    
    对于过拟合，可以采用正则化、降低模型复杂度、增加训练数据等方法来减轻过拟合现象；
    
    对于欠拟合，可以考虑增加模型复杂度、增加特征数量、调整模型参数等方法来提高模型的拟合能力。

**经典的决策树算法**

* **ID3**
  
    根据 “最大信息熵增益” 原则选择划分当前数据集的最好特征。
  
    ID3倾向于选择倾向于可取值数目较多的属性，且要求属性特征必须是离散的，对于长度、密度等连续的特征不能处理。

* **C4.5**
  
    C4.5算法流程与ID3相类似，只不过将信息增益改为**信息增益比**，以解决ID3算法倾向于选择取值较多的属性的问题，并且它可以处理连续型属性(连续型属性离散化处理)。
  
    $$
    Gain\_ratio(S, A) = \frac{IG(S, A)}{H_A(S)} \\\
    H_A(S) = -\sum_{i=1}^n \frac{|D_i|}{|D|} log_2 \frac{|D_i|}{|D|}
    $$

* **C5.0**
  
    C5.0 算法作为 C4.5的改进版，算法原理一致，都是采用信息增益比来进行特征选择，但C5.0适用于处理大数据集，采用 Boosting 方式来提高模型准确率，计算速度较高，对计算内存占用也较少。

* <p id="CART">**CART(Classifacation And Regression Tree)**</p>
  
    CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。
  
    $$
    Gini(S) = \sum_{i=1}^n p(x_i) * (1-p(x_i)) = 1 - \sum_{i=1}^n p(x_i)^2
    $$
    Gini(S)越小，则数据集S的纯度越高。
  
    CART算法建立起的是二叉树，根据某一属性A是否可以取值a，把样本分为了两部分 (属性A可以有大于2个属性取值)。在属性A条件下，样本的基尼系数定义为：
  
    $$
    GiniIndex(D|A=a) = \frac{D_1}{D}Gini(D_1) + \frac{D_2}{D}Gini(D_2)
    $$
  
    选择具有最小的GiniIndex的属性及其属性值作为划分节点，即：
    $$
    \mathop{min}\limits_{A\in AllAttrs} (\mathop{min}\limits_{a\in A} (GiniIndex(D|A=a)))
    $$
  
    相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。

???+ quote "Reference"

    [决策树三种算法比较(ID3、C4.5、CART)](https://blog.csdn.net/qq_43468807/article/details/105969232)

### 2.2 集成学习算法

**“三个臭皮匠，顶个诸葛亮。”**

!!! note inline end "集成学习的思想"

    集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。

在监督学习中，理想的情况是学习得到一个泛化能力强、稳定且在各个方面表现都优良的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型。

* 概率近似正确 (Probably Approximately Correct, PAC)学习
  
    最基本的计算学习理论——以较大的概率学得误差满足预设上限的模型，PAC 学习给出了一个抽象地**刻画机器学习能力**的框架。

* <p id="bagging">Bagging(Bootstrap aggregating)</p>
  
    !!! note "bootstrap"
  
        bootstrap也称自展法、自举法、自助法、靴带法，是统计学习中一种重采样(Resampling)技术，用来估计标准误差、置信区间和偏差。
      
        bootstrap的基本思想是从样本中重抽样并用重抽样数据来推断总体，其一般流程是从原始样本中有放回地抽取一定数量的样本（这个数量通常与原始样本数量相同），计算抽取样本的统计量，重复这个过程多次（一般大于1000）。在小样本上bootstrap表现很好。
      
        由于是对样本集的有放回抽样，因此每次抽取的训练集都不同，而且里面包含重复的训练样本。
      
        对于Bagging算法，一般会随机采集和训练集样本数m一样个数的样本。这样得到的采样集和训练集样本的个数相同，但是样本内容不同。
  
    在Bagging方法中：
  
  * 利用bootstrap方法从整体数据集中采取有放回抽样得到N个数据集；  
  
  * 在每个数据集上学习出一个模型，最后的预测结果利用N个模型的输出得到。  
    
    > 分类问题采用N个模型预测投票的方式;  
    > 回归问题采用N个模型预测平均的方式。
    
    一种bagging方法就是[随机森林](#23)，其用到的弱分类器是CART决策树。

* Boosting
  
    !!! note inline end
  
        Bosting是一种可以应用于许多回归和分类统计学习的通用方法。
  
    Boosting的训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化。对所有基模型预测的结果进行线性综合产生最终的预测结果。
  
    Boosting通过组合许多“弱”分类器来产生一个强大的分类器组。一个弱分类器的性能只是比随机选择好一点，因此它可以被设计的非常简单并且不会有太大的计算花费。

???+ quote "Reference"

    [概率近似正确？！（PAC Learning） - 知乎](https://zhuanlan.zhihu.com/p/44589648)
    
    [集成学习（Ensemble Learning） - 知乎](https://zhuanlan.zhihu.com/p/27689464)
    
    [集成学习算法之Boosting - 知乎](https://zhuanlan.zhihu.com/p/260959204)

### 2.3 随机森林

随机森林(Random Forest, RF)是一种Bagging集成学习算法，它使用多棵决策树(<a href="#CART">CART</a>)构造森林。

单个决策树的表现可能并不那么好，随机森林通过学习得到多个决策树（各个决策树不相同），采用投票法得出结果（即最后哪个结果占比最大，就采用哪个结果）。

**随机森林要点**：

* 通过bootstrap独立采样得到的训练数据集和变量（特征/属性）训练决策树。

* 通过使用输入属性(特征)的**随机子集**中的最佳属性来划分来生成树，而不是考虑全集的最佳划分属性，这可以减少树之间的相关性和泛化误差。
  
    $$
    \mathop{min}\limits_{A\in SubAttrSet} (\mathop{min}\limits_{a\in A} (GiniIndex(D|A=a)))
    $$

* 基尼指数通常在RF中用作最佳分割选择的度量，以最大化类别之间的差异。这正是CART的做法。

* RF在处理大数据集和变量方面具有许多优点，但不会过拟合。

* RF分类器中有两个参数:每个节点的随机子集中活动变量的数量(M)和森林中树的数量(T)。

**RF中决策树的生成规则**：

1. 如果训练集大小为N，对于每棵树而言，使用[bootstrp](#bagging)方法从训练集中的抽取N个训练样本作为该树的训练集。

2. 如果存在M个特征维度(即可用于划分数据集的属性)，则指定数量 m << M，使得在每个节点处，从M中随机选择m个特征维度，并且使用这m个特征维度中最佳特征来分割节点。在森林生长期间，m的值保持不变。

3. 每棵树都尽最大程度的生长，没有剪枝。

**对新样本进行预测**：

* 回归：结果平均值

* 分类：投票法

**随机森林分类效果(错误率)影响因素**：

* 森林中任意两棵树的相关性：相关性越大，错误率越大；

* 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。
  
    减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大，位于中间位置的某个m是最佳值（通常比较大）。选择合适的m是影响随机森林效果的关键。

**随机森林调参（m）**：

* 确定合适的参数取决于一个具体的问题，需要一个调优过程。

* 可以使用[OOB](#OOB)和交叉验证方法，不断训练模型直至OOB稳定下来。

**袋外样本**：

随机森林的一个重要优点是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以直接在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。

在构建每棵树时，我们对训练集使用了不同的bootstrap取样（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），平均大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的袋外样本(Out-of-Bag samples)。

**<p id="OOB">袋外错误率估计**:</p>

随机森林的bootstrap采样特点允许我们进行OOB估计：

1. 对训练集的**每个样本**，计算它作为袋外样本(OOB样本)的决策树（对每个样本约有1/3的树）对它的分类情况；  

2. 以简单多数投票法得到的结果作为分类结果；  

3. 对所有样本计算完毕后，用误分的个数占样本总数的比率作为随机森林的OOB误分率。

oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。这样，就可以通过比较oob误分率来选择一个最好的特征数m。

**变量重要性评估**：

现实情况下，一个数据集中往往有成百上前个特征，如何在其中选择比结果影响最大的那几个特征，以此来缩减建立模型时的特征数是研究比较关心的问题。随机森林变量重要性评估方法大致步骤如下：

1. 计算[OOB错误率](#OOB) $e_1$；

2. 随机排列袋外样本中某个变量x的值(保持其它变量的值不变)，将排列后的样本再次进行OOB错误率估计得到 $e_2$；

3. 计算$importance_x = \sum \frac{e_2 - e_1}{|T|}$, 其中|T|是森林中树的数目。如果x改变之后产生的变化越大，说明x对划分的影响越大，该变量越重要。

**随机森林的特点和优点**：

1. 它是目前最精确的学习算法之一。 对于许多数据集，它产生了一个高度准确的分类器。

2. 它可以在大型数据库上高效运行。

3. 它可以处理数千个输入变量而不需要删除变量。

4. 它能估计出分类中的重要变量有哪些。

5. 随着森林的构建，它产生一个内部无偏估计的泛化误差。

6. 它具有一种有效的估计缺失数据的方法，并在大量缺失数据的情况下保持准确性。

7. 它有用于平衡类总体不平衡数据集误差的方法。

8. 生成的森林可以保存起来供将来用于其他数据。

9. 给出关于变量和类别之间关系的信息。

10. 它计算案例对之间的相近度，用于聚类、定位异常值或(通过缩放)给出数据的有趣视图。

11. 上述功能可以扩展到未标记数据，从而实现无监督聚类、数据视图和离群值检测。

12. 它提供了一种检测变量相互作用的实验方法。

**随机森林的缺点**：

1. 在一些具有噪声类别或进行回归任务的数据集中观察出了过拟合现象。

2. 对于包含不同级别的分类变量的数据集，随机森林倾向于选择具有更多级别的属性。因此对这类数据来说从随机森林得到的变量重要性分数是不可靠的。

???+ quote "Reference"

    [机器学习 - 随机森林 Random Forest） - 知乎](https://blog.csdn.net/weixin_41332009/article/details/113815702)
    
    [随机森林里oob_score以及用oob判断特征重要性的理解](https://blog.csdn.net/MingRachel/article/details/115038730)
