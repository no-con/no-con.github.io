???+ Abstract

    介绍了空间异质性以及一种基于空间异质性的局部空间分析方法——地理加权回归分析(GWR)。
    
    重点掌握空间异质性的概念和GWR的本质，了解带宽优化策略。


## 1. 空间非平稳性/空间异质性

* 辛普森悖论  

    在分组比较中都占优势的一方，在总评中却是不占优势甚至是失势的一方。

* 非平稳性的影响

    如果使用平稳模型对非平稳数据进行建模，可能得到完全错误的结论，模型的残差很有可能是高度空间自相关的。

* 关系随着空间变化而变化的原因

    * 取样造成的：样本的空间位置不同。这不是真正的空间异质性。

    * 本质上不同空间位置具有不同的空间关系。这是真正的空间异质性。

    * 建模失误。

* **空间非平稳性/异质性 (Spatial non-stationarity/Heterogenicity)**

    在空间中的不同位置进行相同的刺激却产生不同的反应/结果。

    * 全局模型：平稳的，与位置无关。

    * 局部模型：全局模型的分解，与位置相关——通常是需要从地理空间数据中提取的特征。
        
* 衡量空间异质性的指标
    
    [空间关联的局部指标(LISA)](../6_SpaCorr/#24-lisa): 局部莫兰指数等。

* 在空间异质性的影响下对变量进行预测
    
    地理加权回归（Geographically weighted regression, GWR）

## 2. GWR的意义

1. GIS具有向局部分析发展的趋势，GWR是局部分析的一种方法。

    局部分析是全局分析的分解，倾向于更详细地了解空间数据。

2. 使GIS与空间分析更好得连接。
    
    促进GIS与空间分析更好地结合的一个重要因素是局部空间统计技术的发展，GWR是近年来发展起来的局部空间统计技术之一。

3. GWR广泛适用于几乎任何形式的空间数据。
    
    "健康"和"财富"之间的空间联系    
    存在/不存在某种疾病    
    房价的决定因素  
    区域发展机制    
    空气质量:AQI, PM2.5    
    城市热岛

4. GWR是一种真正的空间技术。
    
    它既使用地理信息，也使用属性信息。 
    它采用空间加权函数，假设近的地方比远的地方更相似(地理问题)。    
    它的输出是特定于位置的，因此可以进行映射以供进一步的分析。

5. GWR的残差通常要小得多，且残差基本上与空间位置不相关。

6. GWR是“空间显微镜”。
    
    GWR模型可以先验地输入，而不是确定最优带宽(最近邻方法)。   
    可以选择一系列的带宽，并可在不同的平滑水平(就像在显微镜中调整放大系数)检查得到参数表面。  
    不同的细节会表现出不同的空间变化模式，这使得研究人员能够更灵活地发现有趣的空间模式，检验理论，并确定下一步的步骤。

## 3. GWR模型

### 3.1 GWR模型的本质

正如在上一章所介绍的，传统的回归模型如下：
$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_k + \varepsilon
$$
对某个观测值$x_i$:
$$
y_i = \beta_0 + \beta_1 {x_1}_i + \beta_2 {x_2}_i + ... + \beta_k {x_k}_i + \varepsilon_i
$$
GWR提出在不同位置$\beta_k$可以不同, 即$\beta_k$随位置$(u_i, v_i)$变化：
$$
y_i = \beta_0(u_i, v_i) + \beta_1(u_i, v_i) {x_1}_i + \beta_2(u_i, v_i) {x_2}_i + ... + \beta_k(u_i, v_i) {x_k}_i + \varepsilon_i \\\\
$$

相应的，该回归方程需要使用局部加权最小二乘法求解，其中权重与位置相关：
$$
Q = min \sum_{j=1}^n w_{ij}[y_i - \hat \beta_0(u_i, v_i) - \sum_{k=1}^m \hat \beta_k(u_i, v_i)x_{ki}]^2
$$
其中，$w_{ij}$ 表示样本点j到回归分析的i的权重，W也被称为核函数。
对于每个点 $(u_i, v_i)$, 其空间权重矩阵为：
$$
\begin {bmatrix}
w_{i1} & 0 & \cdots & 0 \\\\
0 & w_{i2} & \cdots & 0 \\\\
\vdots & \vdots & \ddots & \cdots \\\\
0 & 0 & \cdots & w_{in} \\\\
\end {bmatrix}
$$
GWR模型参数计算的关键就是获取空间权重$w_{ij}$

### 3.2 常见的空间加权方案

1. 距离阈值

    $$ 
    w_{ij}=
    \begin{cases}
    1, d_{ij} < thershold \\\
    0, otherwise
    \end{cases}
    $$

2. 反距离

    $$
    w_{ij} = \frac{1}{d_{ij}^a}
    $$
    a越大，对点的作用越显著，随距离衰减得越快。

3. **高斯或类高斯函数**

    高斯函数：
    $$
    w_{ij} = e^{-(d_{ij}/b)^2}
    $$
    其中b是带宽，指的就是权重与距离之间函数关系的非负衰减参数。带宽越大，权重随距离的增加衰减的越慢，带宽越小，权重随距离的增加衰减的就快。这个参数与上面反距离中幂函数的参数a的作用是一样，但是与直接的反距离公式不同的是：在这个公式里面，当带宽为0的时候，只有回归点上的权值为1，其他各观测点的权重都无限趋近0，这样来说，回归的过程也就是数据的重新表达而已。而带宽无穷大的时候，所有的观察点权重都无限接近1，那么就变成了全局回归了。

    但是，如果数据非常离散，带来的结果就是有大量的数据距离很远，这种所谓的“长尾效应”会带来大量的计算开销，所以在实际运算中，应用的是类高斯函数来替代高斯计算，把那些没有影响（或者影响很少）的点给截掉，以提高效率。
    
    !!! note inline end 
        高斯或类高斯函数和Bi-square函数在实际计算中是最常用的两种方法。

4. **双重平方函数 (Bi-square function)**
 
    $$ 
    w_{ij}=
    \begin{cases}
    [1-(d_{ij}/b)^2]^2, d_{ij} < thershold \\\
    0, otherwise
    \end{cases}
    $$
    bi-square函数其实是距离阈值法和Gauss函数法的结合。回归点在带宽的范围内，通过高斯单调递减函数计算数据点的权重，超出的部分，权重全部记为0。

### 3.3 自适应加权方案

固定带宽存在的问题:

  * 在数据稀疏的情况下可能会产生较大的估计方差，而在数据密集的情况下会掩盖微妙的局部变化。

  * 在极端情况下，固定方案可能无法在数据过于稀疏而无法满足校准要求的局部区域进行校准(观测值必须多于参数)。

自适应方案根据数据点的密度自动调整带宽：

  * 数据密集的地方带宽小，数据稀疏的地方带宽大。

  * 一个常用的寻找带宽的方法是最近邻点法。

### 3.4 带宽优化策略

* 最小CV分数准则（Least Cross Validation Score）
    
    交叉验证法（Cross Validation）：将样本划分为互补的子集，在一个子集上进行训练（训练集），在另一个自子集上进行验证（验证集或测试集）。

    为减小偶然性，还可以将样本划分为多个子集，进行多轮交叉验证，将结果进行组合（如平均）作为模型的预测性能估计。

    $$
    CV = \frac{1}{n} \sum_{i=1}^n [y_i - \hat y_{\neq i}(b)]^2
    $$
    CV最小时，得到最优模型b。

* 最小赤池信息准则（Least Akaike Information Criterion, AIC）

    AIC建立在信息论的基础上。AIC估计所得模型丢失的信息的相对量:模型丢失的信息越少，模型的质量越高。在估计模型丢失的信息量时，AIC处理模型的拟合优度和模型的简单性之间的权衡。换句话说，AIC处理过拟合风险和欠拟合风险。

    $$
    AIC = 2nln \hat \sigma + nln(2\pi) + n[\frac{n+tr(S)}{n-1-tr(S)}]
    $$
    其中，$\hat \sigma$ 是随机误差方差的最大似然估计，$tr(S)$ 是一个与带宽有关的函数（[帽子矩阵](https://baike.baidu.com/item/%E5%B8%BD%E5%AD%90%E7%9F%A9%E9%98%B5/19112758)的迹）。

### 3.5 GWR检验

* GWR与OLS的对比
    
    * 方差分析表检验（analysis of variance, ANOVA）

    * AIC准则：

        > 经验法则：相比于OLS，GWR模型的AIC降低3就被视为是成功的改进。
    

## 4. GWR的扩展

### 4.1 多尺度加权回归

Multiscale Geographically Weighted Regression (MGWR)。

每个自变量可能在不同的空间尺度上起作用。 GWR不能解释这一点，但MGWR允许每个自变量有不同的邻域(带宽)。 

自变量 $x_i$ 的邻域(带宽)决定了在拟合目标特征的线性回归模型中用于估计该自变量系数 $\beta_i$ 的特征。

### 4.2 其他基于GWR的扩展模型

* Geographically and Temporally Weighted Regression Model, GTWR 时空加权回归模型

* Geographical Neural Network Weighted Regression model, GNNWR 地理神经网络加权回归模型

* Geographically and temporally neural network weighted regression model, GTNNWR 地理时空神经网络加权回归模型

* Geographically convolutional neural network weighted regression model, GCNNWR 地理卷积神经网络加权回归模型
