???+ abstract
    介绍了地学分析中的应用到的传统回归分析方法，重点是二元线性回归。

## 1. 回归分析

### 1.1 什么是回归分析

!!! note inline end
    independent variable: 自变量  
    explanatry variable: 解释变量

    dependent variable: 因变量  
    response variable: 响应变量

回归分析：对因变量（响应变量）与自变量（解释变量）之间因果关系的建模。

回归分析提供了：   

* 变量之间的关系的简化视图  
* 拟合所提供数据之间模型的方法  
* 评价变量重要性和模型正确性的方法  

### 1.2 回归模型的类型

* 根据自变量个数分：二元回归、多元回归；  
* 根据自变量和因变量之间的关系分：线性回归、非线性回归；  
* 是否考虑空间关系：传统（经典）回归、空间回归；  
* 根据回归参数的估计方法：普通最小二乘法（OLS）、最大似然法（ML）、SVM、RF……

### 1.3 回归分析的作用

发现自变量和因变量之间潜在的因果关系。

回归分析的一个重要结果是给出一个方程，允许我们利用自变量x预测因变量y的值。

## 2. 二元线性回归

### 2.1 数学模型

首先假设因变量(y)和自变量(x)之间存在线性关系，然后用一条直线对观察数据集进行拟合，关注变量x对y具有的影响的解释和分析，以及拟合的性质。该直线的一般形式如下：
$$
\hat y = a + bx
$$
其中$x$ 是自变量的观测值，$\hat y$ 是因变量的预测值，$a$ 是截距，$b$ 是斜率。需要注意的是，截距并不总是具有很好的现实意义，自变量取0值时，因变量的观测值可能远远超出根据回归方程计算出来的预测值。

对因变量y的观测值可以表示为：
$$
y = a + bx + e = \hat y + e
$$
其中e称为残差，即 $e = y - \hat y$。

### 2.2 真实回归直线与最佳拟合直线

如果将总体作为样本，那么我们可以得到一条理论上完全真实的回归直线(true regression line)：
$$
y = \alpha + \beta x + \varepsilon
$$
其中 $\alpha$ 和 $\beta$ 分别为真实回归曲线的截距和斜率，$\varepsilon$ 是随机误差。  

但是，由于我们往往能得到的只是总体的一部分样本，因此我们计算出来的回归直线只是基于该样本数据的最佳拟合曲线（best-fitting line），由于样本的选择具有不确定性，它与真实拟合曲线可能具有较大的偏差。

### 2.3 普通最小二乘法

回归分析的目的实际是使得残差平方和最小化，即使 $\sum (y_i - \hat y_i)^2$ 最小，求解该问题的方法为普通最小二乘法（Ordinary Least Squares, OLS）。

残差平方和表示为：
$$
Q = \sum_{i=1}^n (y_i - \hat y_i)^2 = \sum_{i=1}^n (y_i - a - bx_i)^2
$$
要使Q最小，实际就是分别对 a 和 b求偏导，使偏导等于0即可。  
$$
\begin{cases} 
\frac{\partial Q}{\partial a} = -2 \sum_{i=1}^n (y_i - a - bx_i) = 0 \\\\ 
\\\\
\frac{\partial Q}{\partial b} = -2 \sum_{i=1}^n (y_i - a - bx_i)x_i = 0
\end{cases}
$$
整理得：
$$
b = \frac {\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y) }{\sum_{i=1}^n (x_i - \bar x)^2} 
$$
$$
a = \bar y - b \bar x
$$
其中
$$
\bar x = \frac {\sum_{i=1}^n x_i}{n}, \bar y = \frac{\sum_{i=1}^n y_i}{n}
$$

### 2.4 回归方程的评价

* 线性回归决定系数

    也有称为判定系数或拟合优度的，可参考上一章的[回归模型中几个平方和的含义](../6_SpaCorr/#14)。

    $$
    r^2 = \frac{\sum_{i=1}^n (\hat y_i - \bar y)^2}{\sum_{i=1}^n     (y_i-\bar y)^2} = 1 - \frac{\sum_{i=1}^n e_i^2}{(n-1)s_y^2}
    $$

    决定系数范围为[0,1], 决定系数越大，自变量对因变量的解释程度越高，自变量引起的变动占总变动的百分比高。观察点在回归直线附近越密集。

* $r^2$ 的显著性检验（F检验）：
$$
F = \frac {SSR/k}{RSS/(n-k-1)} = \frac{SSR}{RSS/(n-2)} = \frac{r^2(n-2)}{1-r^2}
$$

* 估计标准误差（standard error of estimate）是残差平方和的均方根：
    $$
    s_e = \sqrt{\frac{\sum_{i=1}^n(y_i-\hat y_i)^2}{n-2}}
    $$
    
    $s_e$ 是度量各观测点在回归直线周围分散程度的一个统计量，反映了实际观测值 $y_i$ 与回归估计值 $\hat y_i$ 之间的差异程度。  

    它也是对误差项 $\varepsilon$ 的标准差估计，可以看作在排除了 $x$ 对 $y$ 的线性影响后随机波动大小的一个估计量。$s_e$从实际意义看，反映了用估计的回归方程预测因变量 $y$ 时预测误差的大小，$s_e$ 越小，说明根据回归方程进行预测也就越准确；若各观测点全部落在直线上，则 $s_e=0$，此时用自变量来预测因变量是没有误差的。  

    可见 $s_e$ 也从另一个角度说明了回归直线的拟合程度。

    ???+ quote "Reference"  

        [估计标准误差-国家统计局](https://www.stats.gov.cn/zs/tjll/xgfx/202305/t20230517_1939714.html)

### 2.5 回归系数$\beta$的t检验

零假设$H_0: \beta = 0$

$$
 t = \frac{b-\beta}{s_b} = \frac {b} {s_b}
$$
其中$s_b$是斜率的标准差
$$
s_b = \sqrt{\frac{S_e^2}{(n-1)S_x^2}}
$$

### 2.6 置信区间的计算

* 如果数据中包含的是单个观测值：
$$
\hat y \pm t_{\alpha, df} * S * \sqrt{\frac{1}{n} + \frac{(x_i^* - \bar x)^2}{\sum (x_i - \bar x)^2} + 1} 
$$
    其中：

    $t_{\alpha, df}$ 表示显著性水平为$\alpha$、自由度为df时的两个尾部t值； 

    $S$ 为[估计标准误差](#24)；  

    $X_i^\*$ 为用于预测Y时的特特定观测值X；  

    $X_i$ 为样本观测值。 

* 如果数据集中还包含来自其它数据集算得的平均值，那么将给出比上面的区间稍窄的结果：
    $$
    \hat y \pm t_{\alpha, df} * S * \sqrt{\frac{1}{n} + \frac{(x_i^* - \bar x)^2}{\sum (x_i - \bar x)^2} } 
    $$

### 2.7 二元线性回归分析步骤

1. 建立关系理论

    提出所期待的变量之间的关系，并指定自变量与因变量。

2. 检查数据
    
    构造散点图观察数据关系。

    计算数据之间的相关系数。

3. 指定回归模型

    $y = a + bx + \varepsilon$

4. 进行 $r^2$ 的显著性检验

    指定显著性水平，计算自由度，进行F检验。

5. 检验残差

    构造残差图，观察是否在0两侧均匀分布。

    如果残差分布不均，说明不满足线性回归分析的要求。

6. 得到回归方差

7. 构造预测值的置信区间  

### 2.7 简单介绍广义线性回归

线性回归模型是通过数据去拟合出一条直线，一般公式为：
$$
y = \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_k + \alpha
$$

广义线性回归，就是通过某个激活函数，将线性回归的输出值映射成不同的分布，比如说伯努利分布、泊松分布、伽马分布等。不同的分布对应不同的激活函数，并且通过不同的损失函数进行迭代训练，从而得到不同应用场景下的广义线性回归模型。

!!! note inline end "sigmoid函数"
    $$
    sigmoid(x) = \frac{1} {1+e^{-x}}
    $$

比如，一种广义线性回归——二分类逻辑回归是在线性回归的基础上，通过sigmoid函数对线性回归的输出结果y继续进行映射，使映射后的数据符合伯努利分布，即输出值从连续值y，变为g(y)，其值仅为0和1。

## 3. 二元非线性回归

对于非线性关系，我们可以将它们转换成线性关系进行处理。

* 指数关系($y = de^{bx}$)：

    由 $y^{'} = ln y, x^{'} = x, a = ln d, $ 可得 $y^{'} = a + bx^{'}$。

* 对数关系($y = a + bln x$)：

    由 $y^{'} = y, x^{'} = ln x, $ 可得 $y^{'} = a + bx^{'}$。

* 幂函数关系($y = dx^b$)：
    
    由 $y^{'} = ln y, x^{'} = ln x, a = ln d, $ 可得 $y^{'} = a + bx^{'}$。

    ……

## 4. 多元回归

### 4.1 多元线性回归
有k个自变量的线性回归方程为：

$$
y = \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_k + \alpha
$$

目的是找到使$\sum_{i=1}^n (y - \alpha - \beta_1 {x_1}_i - \beta_2 {x_2}_i - ...  \beta_k {x_k}_i)^2 $ 最小的回归系数组合。

### 4.2 多重共线性

进行多元线性回归分析时，各个自变量之间的相关性应尽可能小。不过，在一般情况下，许多参与模型的自变量之间往往存在着很强的相关性，这将对预测模型产生很大的影响。

多重共线性就是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。